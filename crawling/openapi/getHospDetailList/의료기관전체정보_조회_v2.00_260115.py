"""
ì˜ë£Œê¸°ê´€ ì „ì²´ì •ë³´ ì¡°íšŒ API í˜¸ì¶œ ìŠ¤í¬ë¦½íŠ¸ v2.00
================================================================================
ì‘ì„±ì¼: 2026-01-15
ëª©ì : ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì› ì˜ë£Œê¸°ê´€ë³„ìƒì„¸ì •ë³´ì„œë¹„ìŠ¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ 
      11ê°œ ëª¨ë“  ì •ë³´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
ì…ë ¥: ë³‘ì›ê¸°ë³¸ëª©ë¡ CSV íŒŒì¼ (ì•”í˜¸í™”ëœ ìš”ì–‘ê¸°í˜¸ í¬í•¨)
ì¶œë ¥: ë³‘ì› ì „ì²´ì •ë³´ CSV íŒŒì¼ (11ê°œ API ì‘ë‹µ í†µí•©)
================================================================================
"""

import requests
import json
from typing import Dict, List, Optional
import pandas as pd
from datetime import datetime
import time
import os
from pathlib import Path
from urllib.parse import quote

# ============================================================================
# ì„¤ì • (Configuration)
# ============================================================================

# API ê¸°ë³¸ ì •ë³´
API_BASE_URL = "http://apis.data.go.kr/B551182/MadmDtlInfoService2.7"

# 11ê°œ API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
API_ENDPOINTS = {
    'eqp': {
        'operation': 'getEqpInfo2.7',
        'name': 'ì‹œì„¤ì •ë³´',
        'description': 'ì˜ë£Œê¸°ê´€ì˜ ì‹œì„¤ í˜„í™©(ë³‘ìƒ ìˆ˜ ë“±)'
    },
    'dtl': {
        'operation': 'getDtlInfo2.7',
        'name': 'ì„¸ë¶€ì •ë³´',
        'description': 'ì˜ë£Œê¸°ê´€ì˜ ê¸°ë³¸ ë° ìƒì„¸ í˜„í™©'
    },
    'dgsbjt': {
        'operation': 'getDgsbjtInfo2.7',
        'name': 'ì§„ë£Œê³¼ëª©ì •ë³´',
        'description': 'ê°œì„¤ëœ ì§„ë£Œê³¼ëª© ì •ë³´'
    },
    'trnsprt': {
        'operation': 'getTrnsprtInfo2.7',
        'name': 'êµí†µì •ë³´',
        'description': 'ì£¼ë³€ êµí†µìˆ˜ë‹¨ ì •ë³´'
    },
    'medoft': {
        'operation': 'getMedOftInfo2.7',
        'name': 'ì˜ë£Œì¥ë¹„ì •ë³´',
        'description': 'ë³´ìœ  ì˜ë£Œ ì¥ë¹„ í˜„í™©'
    },
    'foepaddc': {
        'operation': 'getFoepAddcInfo2.7',
        'name': 'ì‹ëŒ€ê°€ì‚°ì •ë³´',
        'description': 'ì…ì› í™˜ì ì‹ì‚¬ ì œê³µ ê°€ì‚° ì •ë³´'
    },
    'nursiggrd': {
        'operation': 'getNursigGrdInfo2.7',
        'name': 'ê°„í˜¸ë“±ê¸‰ì •ë³´',
        'description': 'ê°„í˜¸ ë“±ê¸‰ ì •ë³´'
    },
    'spcldiag': {
        'operation': 'getSpclDiagInfo2.7',
        'name': 'íŠ¹ìˆ˜ì§„ë£Œì •ë³´',
        'description': 'ì „ë¬¸ ì§„ë£Œ ê°€ëŠ¥ ë¶„ì•¼'
    },
    'spclhosp': {
        'operation': 'getSpclHospAsgFldList2.7',
        'name': 'ì „ë¬¸ë³‘ì›ì§€ì •ë¶„ì•¼',
        'description': 'ë³´ê±´ë³µì§€ë¶€ ì§€ì • ì „ë¬¸ë³‘ì› ë¶„ì•¼'
    },
    'spcsbtj': {
        'operation': 'getSpcSbtjTsdrInfo2.7',
        'name': 'ì „ë¬¸ê³¼ëª©ë³„ì „ë¬¸ì˜ìˆ˜',
        'description': 'ì§„ë£Œ ê³¼ëª©ë³„ ì „ë¬¸ì˜ ì¸ì› ìˆ˜'
    },
    'etchst': {
        'operation': 'getEtcHstInfo2.7',
        'name': 'ê¸°íƒ€ì¸ë ¥ìˆ˜ì •ë³´',
        'description': 'ì•½ì‚¬, ë¬¼ë¦¬ì¹˜ë£Œì‚¬ ë“± ì˜ë£Œ ì¸ë ¥ í˜„í™©'
    }
}

# ì¸ì¦í‚¤ ì„¤ì •
SERVICE_KEY = "Bk8LikYxwbpxf1OKF0mYYonK9RNmYo/mmgtNsZ41rRNxMuIh5s7RgflEXp+Xwp3R0FDR2j01gx62Hc++Jzc2pw=="# ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì› ì˜ë£Œê¸°ê´€ë³„ìƒì„¸ì •ë³´ì„œë¹„ìŠ¤ ì¸ì¦í‚¤(ë””ì½”ë”)
USE_ENCODED_KEY = False

# ì¬ì‹œë„ ì„¤ì •
MAX_RETRIES = 3
RETRY_DELAY = 1

# íƒ€ì„ì•„ì›ƒ ì„¤ì •
CONNECT_TIMEOUT = 10
READ_TIMEOUT = 60

# ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
ENABLE_CHECKPOINT = True
CHECKPOINT_INTERVAL = 5

# ì¤‘ê°„ ì €ì¥ ì„¤ì • (ì§„í–‰ ì¤‘ì¸ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ CSVë¡œ ì €ì¥)
ENABLE_INTERIM_SAVE = True  # ì¤‘ê°„ ì €ì¥ í™œì„±í™”
INTERIM_SAVE_INTERVAL = 10  # ì¤‘ê°„ ì €ì¥ ê°„ê²© (ê±´ìˆ˜)

# ì…ë ¥ íŒŒì¼ ì„¤ì •
INPUT_CSV_FILE = r"D:\git_gb4pro\crawling\openapi\getHospDetailList\data\ì„œìš¸_ê°•ë‚¨êµ¬_í”¼ë¶€ê³¼_20260115_212757.csv" # ì„œìš¸_ê°•ë‚¨êµ¬_í”¼ë¶€ê³¼_20260115_212757.csv input íŒŒì¼ ìœ„ì¹˜
YKIHO_COLUMN = None  # ìë™ íƒì§€

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
TEST_MODE = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ì†ŒëŸ‰ ë°ì´í„°ë§Œ ì²˜ë¦¬
MAX_TEST_RECORDS = 3  # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ ì²˜ë¦¬í•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜

# ============================================================================
# API í˜¸ì¶œ í•¨ìˆ˜
# ============================================================================

def call_single_api(
    service_key: str,
    operation: str,
    ykiho: str,
    use_encoded_key: bool = False,
    max_retries: int = MAX_RETRIES,
    retry_delay: int = RETRY_DELAY
) -> Optional[Dict]:
    """
    ë‹¨ì¼ API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    
    Parameters:
    -----------
    service_key : str
        ê³µê³µë°ì´í„°í¬í„¸ ì¸ì¦í‚¤
    operation : str
        API operation ì´ë¦„ (ì˜ˆ: getDtlInfo2.7)
    ykiho : str
        ì•”í˜¸í™”ëœ ìš”ì–‘ê¸°í˜¸
    use_encoded_key : bool
        ì¸ì¦í‚¤ ì¸ì½”ë”© ì—¬ë¶€
    max_retries : int
        ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    retry_delay : int
        ì´ˆê¸° ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„
    
    Returns:
    --------
    dict or None
        API ì‘ë‹µ ë°ì´í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    
    # API URL êµ¬ì„±
    api_url = f"{API_BASE_URL}/{operation}"
    
    # ìš”ì²­ íŒŒë¼ë¯¸í„°
    params = {
        'ykiho': ykiho,
        '_type': 'json'
    }
    
    # API í‚¤ ì²˜ë¦¬
    if use_encoded_key:
        encoded_key = quote(service_key, safe='')
        api_url = f"{api_url}?ServiceKey={encoded_key}"
    else:
        params['ServiceKey'] = service_key
    
    # ì¬ì‹œë„ ë¡œì§
    last_exception = None
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                wait_time = retry_delay * (2 ** (attempt - 1))
                time.sleep(wait_time)
            
            response = requests.get(
                api_url,
                params=params,
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)
            )
            response.raise_for_status()
            
            data = response.json()
            header = data['response']['header']
            
            if header['resultCode'] != '00':
                # ë°ì´í„° ì—†ìŒì€ ì •ìƒ ì²˜ë¦¬ (None ë°˜í™˜)
                if header['resultCode'] == '3':
                    return None
                raise Exception(f"API ì˜¤ë¥˜ [{header['resultCode']}]: {header['resultMsg']}")
            
            return data
            
        except requests.exceptions.Timeout:
            last_exception = Exception(f"API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼")
            if attempt < max_retries:
                continue
        except requests.exceptions.ConnectionError:
            last_exception = Exception("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜")
            if attempt < max_retries:
                continue
        except requests.exceptions.HTTPError as e:
            last_exception = Exception(f"HTTP ì˜¤ë¥˜: {e}")
            break
        except KeyError as e:
            last_exception = Exception(f"ì‘ë‹µ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {e}")
            break
        except Exception as e:
            last_exception = Exception(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            if attempt < max_retries:
                continue
    
    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
    return None


def flatten_dict_with_prefix(data: Dict, prefix: str, parent_key: str = '') -> Dict:
    """
    ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”í•˜ê³  ì ‘ë‘ì‚¬ ì¶”ê°€
    
    Parameters:
    -----------
    data : dict
        í‰íƒ„í™”í•  ë”•ì…”ë„ˆë¦¬
    prefix : str
        ì¶”ê°€í•  ì ‘ë‘ì‚¬
    parent_key : str
        ë¶€ëª¨ í‚¤ (ì¬ê·€ í˜¸ì¶œìš©)
    
    Returns:
    --------
    dict
        í‰íƒ„í™”ëœ ë”•ì…”ë„ˆë¦¬
    """
    items = []
    for k, v in data.items():
        new_key = f"{prefix}_{parent_key}_{k}" if parent_key else f"{prefix}_{k}"
        
        if isinstance(v, dict):
            items.extend(flatten_dict_with_prefix(v, prefix, k).items())
        elif isinstance(v, list):
            # ë¦¬ìŠ¤íŠ¸ëŠ” JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            items.append((new_key, json.dumps(v, ensure_ascii=False)))
        else:
            items.append((new_key, v))
    
    return dict(items)


def get_hospital_all_info(
    service_key: str,
    use_encoded_key: bool,
    ykiho: str,
    hospital_name: str = '',
    hospital_addr: str = ''
) -> Dict:
    """
    ë‹¨ì¼ ë³‘ì›ì˜ ëª¨ë“  ì •ë³´ ì¡°íšŒ (11ê°œ API í˜¸ì¶œ)
    
    Parameters:
    -----------
    service_key : str
        ê³µê³µë°ì´í„°í¬í„¸ ì¸ì¦í‚¤
    use_encoded_key : bool
        ì¸ì¦í‚¤ ì¸ì½”ë”© ì—¬ë¶€
    ykiho : str
        ì•”í˜¸í™”ëœ ìš”ì–‘ê¸°í˜¸
    hospital_name : str
        ë³‘ì›ëª… (ì›ë³¸ ë°ì´í„°)
    hospital_addr : str
        ë³‘ì› ì£¼ì†Œ (ì›ë³¸ ë°ì´í„°)
    
    Returns:
    --------
    dict
        í†µí•©ëœ ë³‘ì› ì •ë³´
    """
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    result = {
        'ì›ë³¸_ê¸°ê´€ì½”ë“œ': ykiho,
        'ì›ë³¸_ë³‘ì›ëª…': hospital_name,
        'ì›ë³¸_ì£¼ì†Œ': hospital_addr
    }
    
    # API í˜¸ì¶œ ì„±ê³µ ì¹´ìš´í„°
    success_count = 0
    
    # 11ê°œ API ìˆœì°¨ í˜¸ì¶œ
    for prefix, endpoint_info in API_ENDPOINTS.items():
        operation = endpoint_info['operation']
        name = endpoint_info['name']
        
        try:
            # API í˜¸ì¶œ
            data = call_single_api(
                service_key=service_key,
                operation=operation,
                ykiho=ykiho,
                use_encoded_key=use_encoded_key
            )
            
            if data is None:
                print(f"    [{prefix}] {name}: ë°ì´í„° ì—†ìŒ")
                continue
            
            # ì‘ë‹µ ë°”ë”” ì¶”ì¶œ
            body = data['response']['body']
            items = body.get('items', {})
            
            if not items or isinstance(items, str):
                print(f"    [{prefix}] {name}: ë°ì´í„° ì—†ìŒ")
                continue
            
            item_data = items.get('item', {})
            
            if not item_data or isinstance(item_data, str):
                print(f"    [{prefix}] {name}: ë°ì´í„° ì—†ìŒ")
                continue
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì‚¬ìš©
            if isinstance(item_data, list):
                if len(item_data) > 0:
                    item_data = item_data[0]
                else:
                    print(f"    [{prefix}] {name}: ë¹ˆ ë¦¬ìŠ¤íŠ¸")
                    continue
            
            # ë”•ì…”ë„ˆë¦¬ í‰íƒ„í™” ë° ì ‘ë‘ì‚¬ ì¶”ê°€
            flattened = flatten_dict_with_prefix(item_data, prefix)
            result.update(flattened)
            
            success_count += 1
            print(f"    [{prefix}] {name}: ì„±ê³µ ({len(flattened)}ê°œ í•„ë“œ)")
            
        except Exception as e:
            print(f"    [{prefix}] {name}: ì˜¤ë¥˜ - {e}")
            continue
        
        # API í˜¸ì¶œ ê°„ê²©
        time.sleep(0.1)
    
    print(f"    => ì´ {success_count}/{len(API_ENDPOINTS)}ê°œ API ì„±ê³µ")
    
    return result


# ============================================================================
# ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def load_hospital_list_from_csv(filename: str, ykiho_column: str = None) -> tuple:
    """CSV íŒŒì¼ì—ì„œ ë³‘ì› ëª©ë¡ ì½ê¸°"""
    print(f"[CSV ì½ê¸°] {filename}")
    
    encodings = ['cp949', 'utf-8-sig', 'utf-8', 'euc-kr']
    df = None
    last_error = None
    
    for encoding in encodings:
        try:
            df = pd.read_csv(filename, encoding=encoding)
            print(f"  - ì¸ì½”ë”©: {encoding} (ì„±ê³µ)")
            break
        except (UnicodeDecodeError, Exception) as e:
            last_error = e
            continue
    
    if df is None:
        raise Exception(f"CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
    
    print(f"  - ì´ {len(df)}ê±´")
    print(f"  - ì»¬ëŸ¼: {', '.join(df.columns.tolist()[:5])}...")
    
    # ìš”ì–‘ê¸°í˜¸ ì»¬ëŸ¼ ìë™ íƒì§€
    if ykiho_column is None:
        possible_columns = ['ykiho', 'ì•”í˜¸í™”ìš”ì–‘ê¸°í˜¸', 'ìš”ì–‘ê¸°í˜¸', 'YKIHO', 'ykiho_enc']
        for col in possible_columns:
            if col in df.columns:
                ykiho_column = col
                print(f"  - ìš”ì–‘ê¸°í˜¸ ì»¬ëŸ¼ ìë™ íƒì§€: {ykiho_column}")
                break
        
        if ykiho_column is None:
            raise Exception(f"ìš”ì–‘ê¸°í˜¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ì»¬ëŸ¼: {df.columns.tolist()}")
    
    if ykiho_column not in df.columns:
        raise Exception(f"ì»¬ëŸ¼ '{ykiho_column}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return df, ykiho_column


def save_checkpoint(data: Dict, checkpoint_file: str):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    try:
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[ì²´í¬í¬ì¸íŠ¸ ì €ì¥] {checkpoint_file}")
    except Exception as e:
        print(f"[ê²½ê³ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_checkpoint(checkpoint_file: str) -> Optional[Dict]:
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if not os.path.exists(checkpoint_file):
        return None
    
    try:
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"[ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ] {checkpoint_file}")
        print(f"  - ì´ì „ ì§„í–‰: {data.get('processed_count', 0)}ê±´ ì²˜ë¦¬ ì™„ë£Œ")
        return data
    except Exception as e:
        print(f"[ê²½ê³ ] ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def generate_metadata_markdown(df: pd.DataFrame, csv_filename: str):
    """ë©”íƒ€ë°ì´í„° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±"""
    md_filename = csv_filename.replace('.csv', '.md')
    
    with open(md_filename, 'w', encoding='utf-8') as f:
        # ì œëª©
        f.write(f"# ë³‘ì› ì „ì²´ì •ë³´ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ\n\n")
        f.write(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**ë°ì´í„° íŒŒì¼**: `{Path(csv_filename).name}`\n\n")
        
        # ê¸°ë³¸ ì •ë³´
        f.write("## ğŸ“Š ë°ì´í„° ê°œìš”\n\n")
        f.write(f"- **ì´ ë ˆì½”ë“œ ìˆ˜**: {len(df):,}ê±´\n")
        f.write(f"- **ì´ ì»¬ëŸ¼ ìˆ˜**: {len(df.columns)}ê°œ\n\n")
        
        # APIë³„ ì»¬ëŸ¼ ìˆ˜ ë¶„ì„
        f.write("## ğŸ“‹ APIë³„ ìˆ˜ì§‘ ì •ë³´\n\n")
        api_column_counts = {}
        for prefix in API_ENDPOINTS.keys():
            cols = [col for col in df.columns if col.startswith(f"{prefix}_")]
            api_column_counts[prefix] = len(cols)
        
        f.write("| API | ì •ë³´ í•­ëª© | ì»¬ëŸ¼ ìˆ˜ |\n")
        f.write("|-----|----------|--------|\n")
        for prefix, endpoint_info in API_ENDPOINTS.items():
            count = api_column_counts.get(prefix, 0)
            f.write(f"| `{prefix}` | {endpoint_info['name']} | {count}ê°œ |\n")
        f.write("\n")
        
        # ê²°ì¸¡ì¹˜ ë¶„ì„
        f.write("## ğŸ” ì£¼ìš” ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ë¶„ì„\n\n")
        important_cols = ['ì›ë³¸_ê¸°ê´€ì½”ë“œ', 'ì›ë³¸_ë³‘ì›ëª…', 'ì›ë³¸_ì£¼ì†Œ']
        important_cols += [col for col in df.columns if col.startswith('dtl_')][:10]
        
        missing_data = df[important_cols].isnull().sum()
        missing_pct = (missing_data / len(df) * 100).round(2)
        missing_df = pd.DataFrame({
            'ì»¬ëŸ¼ëª…': missing_data.index,
            'ê²°ì¸¡ì¹˜ ìˆ˜': missing_data.values,
            'ê²°ì¸¡ì¹˜ ë¹„ìœ¨(%)': missing_pct.values
        })
        
        f.write(missing_df.to_markdown(index=False))
        f.write("\n\n")
    
    print(f"[ë©”íƒ€ë°ì´í„° ìƒì„±] {md_filename}")


def save_to_csv(items: List[Dict], filename: str, generate_metadata: bool = True):
    """CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not items:
        print("[ê²½ê³ ] ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(items)
    
    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
    priority_columns = ['ì›ë³¸_ê¸°ê´€ì½”ë“œ', 'ì›ë³¸_ë³‘ì›ëª…', 'ì›ë³¸_ì£¼ì†Œ']
    existing_priority = [col for col in priority_columns if col in df.columns]
    other_columns = [col for col in df.columns if col not in existing_priority]
    final_columns = existing_priority + other_columns
    df = df[final_columns]
    
    # CSV ì €ì¥
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"[ì €ì¥ ì™„ë£Œ] {filename} ({len(df)}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼)")
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (ì„ íƒì )
    if generate_metadata:
        generate_metadata_markdown(df, filename)


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=" * 80)
    print("ì˜ë£Œê¸°ê´€ ì „ì²´ì •ë³´ ì¡°íšŒ í”„ë¡œê·¸ë¨ v2.00")
    print("=" * 80)
    print()
    
    if TEST_MODE:
        print(f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ìµœëŒ€ {MAX_TEST_RECORDS}ê±´ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        print()
    
    # ì¸ì¦í‚¤ í™•ì¸
    if SERVICE_KEY == "ì—¬ê¸°ì—_ë°œê¸‰ë°›ì€_ë””ì½”ë”©_ì¸ì¦í‚¤ë¥¼_ì…ë ¥í•˜ì„¸ìš”":
        print("[ì˜¤ë¥˜] ì¸ì¦í‚¤ë¥¼ ì„¤ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ì…ë ¥ íŒŒì¼ ì½ê¸°
    try:
        hospital_df, detected_ykiho_column = load_hospital_list_from_csv(
            INPUT_CSV_FILE,
            ykiho_column=YKIHO_COLUMN
        )
        ykiho_column = detected_ykiho_column
    except Exception as e:
        print(f"[ì˜¤ë¥˜] CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
    if TEST_MODE:
        hospital_df = hospital_df.head(MAX_TEST_RECORDS)
        print(f"[í…ŒìŠ¤íŠ¸ ëª¨ë“œ] {len(hospital_df)}ê±´ìœ¼ë¡œ ì œí•œ")
        print()
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_file = f"checkpoint_all_{timestamp}.json"
    
    # ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì„¤ì •
    script_dir = Path(__file__).parent
    output_dir = script_dir / "data"
    output_dir.mkdir(exist_ok=True)
    interim_csv_file = output_dir / f"ë³‘ì›ì „ì²´ì •ë³´_ì§„í–‰ì¤‘_{timestamp}.csv"
    
    # ì´ì „ ì§„í–‰ìƒí™© ë¡œë“œ
    all_items = []
    processed_indices = set()
    start_index = 0
    
    if ENABLE_CHECKPOINT:
        checkpoint_data = load_checkpoint(checkpoint_file)
        if checkpoint_data:
            all_items = checkpoint_data.get('items', [])
            processed_indices = set(checkpoint_data.get('processed_indices', []))
            start_index = checkpoint_data.get('last_index', 0) + 1
            print(f"[ì¬ê°œ] ì¸ë±ìŠ¤ {start_index}ë¶€í„° ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            print()
    
    total_count = len(hospital_df)
    start_time = time.time()
    
    try:
        for idx in range(start_index, total_count):
            if idx in processed_indices:
                continue
            
            row = hospital_df.iloc[idx]
            ykiho = row[ykiho_column]
            
            # ìš”ì–‘ê¸°í˜¸ ìœ íš¨ì„± í™•ì¸
            if pd.isna(ykiho) or str(ykiho).strip() == '':
                print(f"[ê²½ê³ ] ì¸ë±ìŠ¤ {idx}: ìš”ì–‘ê¸°í˜¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                processed_indices.add(idx)
                continue
            
            # ì›ë³¸ ë°ì´í„° ì¶”ì¶œ
            hospital_name = row.get('ë³‘ì›ëª…', row.get('yadmNm', ''))
            hospital_addr = row.get('ì£¼ì†Œ', row.get('addr', ''))
            
            # ì§„í–‰ë¥  ê³„ì‚°
            processed_count = len(processed_indices)
            progress_pct = (processed_count / total_count * 100) if total_count > 0 else 0
            elapsed_time = time.time() - start_time
            
            if processed_count > 0 and elapsed_time > 0:
                items_per_sec = processed_count / elapsed_time
                remaining_items = total_count - processed_count
                eta_seconds = remaining_items / items_per_sec if items_per_sec > 0 else 0
                eta_str = f", ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {int(eta_seconds)}ì´ˆ"
            else:
                eta_str = ""
            
            print(f"[ì§„í–‰] {processed_count}/{total_count}ê±´ ({progress_pct:.1f}%){eta_str}")
            print(f"  - ì¸ë±ìŠ¤ {idx}: {hospital_name}")
            
            # 11ê°œ API í˜¸ì¶œ
            hospital_info = get_hospital_all_info(
                service_key=SERVICE_KEY,
                use_encoded_key=USE_ENCODED_KEY,
                ykiho=ykiho,
                hospital_name=hospital_name,
                hospital_addr=hospital_addr
            )
            
            all_items.append(hospital_info)
            processed_indices.add(idx)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (JSON)
            if ENABLE_CHECKPOINT and len(processed_indices) % CHECKPOINT_INTERVAL == 0:
                checkpoint_data = {
                    'last_index': idx,
                    'processed_count': len(processed_indices),
                    'processed_indices': list(processed_indices),
                    'total_count': total_count,
                    'timestamp': datetime.now().isoformat(),
                    'items': all_items
                }
                save_checkpoint(checkpoint_data, checkpoint_file)
            
            # ì¤‘ê°„ ì €ì¥ (CSV) - ì§„í–‰ ì¤‘ì¸ ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ CSVë¡œ ì €ì¥
            if ENABLE_INTERIM_SAVE and len(processed_indices) % INTERIM_SAVE_INTERVAL == 0:
                print(f"[ì¤‘ê°„ ì €ì¥] {len(all_items)}ê±´ ì €ì¥ ì¤‘...")
                save_to_csv(all_items, str(interim_csv_file), generate_metadata=False)
            
            print()
        
        print(f"[ì™„ë£Œ] ì´ ì²˜ë¦¬: {len(processed_indices)}ê±´")
        
        # ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ
        if ENABLE_CHECKPOINT and os.path.exists(checkpoint_file):
            try:
                os.remove(checkpoint_file)
                print(f"[ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ] {checkpoint_file}")
            except:
                pass
        
        # ìµœì¢… CSV ì €ì¥
        if all_items:
            final_csv_file = output_dir / f"ë³‘ì›ì „ì²´ì •ë³´_{timestamp}.csv"
            print(f"\n[ìµœì¢… ì €ì¥ ì‹œì‘]")
            save_to_csv(all_items, str(final_csv_file), generate_metadata=True)
            
            # ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì‚­ì œ (ìµœì¢… íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ)
            if ENABLE_INTERIM_SAVE and interim_csv_file.exists():
                try:
                    interim_csv_file.unlink()
                    print(f"[ì¤‘ê°„ ì €ì¥ íŒŒì¼ ì‚­ì œ] {interim_csv_file.name}")
                except:
                    pass
        
    except Exception as e:
        print(f"[ì˜¤ë¥˜ ë°œìƒ] {e}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if ENABLE_CHECKPOINT:
            checkpoint_data = {
                'last_index': idx if 'idx' in locals() else 0,
                'processed_count': len(processed_indices),
                'processed_indices': list(processed_indices),
                'total_count': total_count,
                'timestamp': datetime.now().isoformat(),
                'error': str(e),
                'items': all_items
            }
            save_checkpoint(checkpoint_data, checkpoint_file)
            print(f"[ì˜¤ë¥˜] ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰ë©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
